sielu
We developed a novel activation function known as sielu, used in a LSTM deep neural network as an activation for comparative study with the existing deep learning model and machine learning model. Here, we used 150 nodes in input layer, 50 nodes in hidden layer with 0.2% of dropout and MSE loss function, “adam” optimizer. 500 epochs were run for training models having SiELU as activation function. The graph of accuracy vs. validation is plotted in figure 4.1.1(g). After testing the mode, we obtained an accuracy of 96.33%. Details of confusion matrix developed (TP, TN, FP and FN) and results (accuracy, precision, Recall, F1-score, specificity and MCC